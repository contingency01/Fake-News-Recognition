{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "\n",
    "**Goal:** Classify a document *d* into one of *K* classes $C_k$, given its words.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Bayes' Theorem\n",
    "\n",
    "$$\n",
    "P(C_k \\mid d) = \\frac{P(d \\mid C_k) \\cdot P(C_k)}{P(d)}\n",
    "$$\n",
    "\n",
    "Since  $P(d)$ is constant across classes, we use:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{C_k} \\left[ P(d \\mid C_k) \\cdot P(C_k) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multinomial Model\n",
    "\n",
    "- Represent document *d* as a vector of word counts:  \n",
    "- $ d = n_1, n_2, \\dots, n_V $\n",
    "- $V$ is the size of the vocabulary.\n",
    "- $n_i$ is the number of times word $w_i$ appears in document *d*.\n",
    "\n",
    "$$\n",
    "P(d \\mid C_k) = \\frac{N_d!}{\\prod_{i=1}^{V} n_i!} \\prod_{i=1}^{V} \\left( P(w_i \\mid C_k) \\right)^{n_i}\n",
    "$$\n",
    "\n",
    "In practice, we ignore the multinomial coefficient and compute:\n",
    "\n",
    "$$\n",
    "\\log P(d \\mid C_k) = \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Priors\n",
    "\n",
    "$$\n",
    "P(C_k) = \\frac{\\text{Number of documents in class } C_k}{\\text{Total number of documents}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Likelihoods with Smoothing\n",
    "\n",
    "$$\n",
    "P(w_i \\mid C_k) = \\frac{N_{ik} + \\alpha}{N_k + \\alpha V}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N_{ik}$ = number of times word $w_i$ occurs in documents of class $C_k$\n",
    "- $N_k$ = total number of words in documents of class $C_k$\n",
    "- $\\alpha$ = smoothing parameter (usually $\\alpha = 1$, Laplace smoothing)\n",
    "- $V$ = size of vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Prediction Rule\n",
    "\n",
    "For a given document *d*, compute for each class:\n",
    "\n",
    "$$\n",
    "\\log P(C_k) + \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "Choose the class with the highest score:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C_k} \\left[ \\log P(C_k) + \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary of Training Steps\n",
    "\n",
    "1. Compute $P(C_k)$ for each class.\n",
    "2. For each word $w_i$ and each class $C_k$, compute $P(w_i \\mid C_k)$ with smoothing.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary of Prediction Steps\n",
    "\n",
    "1. For a new document *d*, compute the score for each class.\n",
    "2. Choose the class with the highest score.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
