{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "\n",
    "**Goal:** Classify a document *d* into one of *K* classes $C_k$, given its words.\n",
    "\n",
    "\n",
    "## 1. Bayes' Theorem\n",
    "\n",
    "$$\n",
    "P(C_k \\mid d) = \\frac{P(d \\mid C_k) \\cdot P(C_k)}{P(d)}\n",
    "$$\n",
    "\n",
    "Since  $P(d)$ is constant across classes, we use:\n",
    "\n",
    "$$\n",
    "\\arg\\max_{C_k} \\left[ P(d \\mid C_k) \\cdot P(C_k) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## 2. Multinomial Model\n",
    "\n",
    "- Represent document *d* as a vector of word counts:  \n",
    "- $ d = n_1, n_2, \\dots, n_V $\n",
    "- $V$ is the size of the vocabulary.\n",
    "- $n_i$ is the number of times word $w_i$ appears in document *d*.\n",
    "\n",
    "$$\n",
    "P(d \\mid C_k) = \\frac{N_d!}{\\prod_{i=1}^{V} n_i!} \\prod_{i=1}^{V} \\left( P(w_i \\mid C_k) \\right)^{n_i}\n",
    "$$\n",
    "\n",
    "In practice, we ignore the multinomial coefficient and compute:\n",
    "\n",
    "$$\n",
    "\\log P(d \\mid C_k) = \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "\n",
    "## 3. Priors\n",
    "\n",
    "$$\n",
    "P(C_k) = \\frac{\\text{Number of documents in class } C_k}{\\text{Total number of documents}}\n",
    "$$\n",
    "\n",
    "\n",
    "## 4. Likelihoods with Smoothing\n",
    "\n",
    "$$\n",
    "P(w_i \\mid C_k) = \\frac{N_{ik} + \\alpha}{N_k + \\alpha V}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N_{ik}$ = number of times word $w_i$ occurs in documents of class $C_k$\n",
    "- $N_k$ = total number of words in documents of class $C_k$\n",
    "- $\\alpha$ = smoothing parameter (usually $\\alpha = 1$, Laplace smoothing)\n",
    "- $V$ = size of vocabulary\n",
    "\n",
    "\n",
    "## 5. Prediction Rule\n",
    "\n",
    "For a given document *d*, compute for each class:\n",
    "\n",
    "$$\n",
    "\\log P(C_k) + \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "Choose the class with the highest score:\n",
    "\n",
    "$$\n",
    "\\hat{C} = \\arg\\max_{C_k} \\left[ \\log P(C_k) + \\sum_{i=1}^{V} n_i \\cdot \\log P(w_i \\mid C_k) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## 6. Summary of Training Steps\n",
    "\n",
    "1. Compute $P(C_k)$ for each class.\n",
    "2. For each word $w_i$ and each class $C_k$, compute $P(w_i \\mid C_k)$ with smoothing.\n",
    "\n",
    "\n",
    "## 7. Summary of Prediction Steps\n",
    "\n",
    "1. For a new document *d*, compute the score for each class.\n",
    "2. Choose the class with the highest score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "library(ggplot2)\n",
    "library(tidytext)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training and testing datasets\n",
    "train_data = read.csv(\"data/train.csv\")\n",
    "test_data = read.csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Divide the training dataset into training and validation set. 20% of the data will be used for validation.\n",
    "\n",
    "set.seed(123)  # For reproducibility\n",
    "\n",
    "train_indices = sample(1:nrow(train_data), size = 0.8 * nrow(train_data))\n",
    "train_set = train_data[train_indices, ]\n",
    "validation_data = train_data[-train_indices, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"--------- Training Data Summary ---------\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 x 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>num_rows</th><th scope=col>num_cols</th><th scope=col>num_missing</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>10240</td><td>3</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 x 3\n",
       "\\begin{tabular}{lll}\n",
       " num\\_rows & num\\_cols & num\\_missing\\\\\n",
       " <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t 10240 & 3 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 x 3\n",
       "\n",
       "| num_rows &lt;int&gt; | num_cols &lt;int&gt; | num_missing &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 10240 | 3 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  num_rows num_cols num_missing\n",
       "1 10240    3        0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"--------- Validation Set Summary ---------\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 x 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>num_rows</th><th scope=col>num_cols</th><th scope=col>num_missing</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>2048</td><td>3</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 x 3\n",
       "\\begin{tabular}{lll}\n",
       " num\\_rows & num\\_cols & num\\_missing\\\\\n",
       " <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t 2048 & 3 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 x 3\n",
       "\n",
       "| num_rows &lt;int&gt; | num_cols &lt;int&gt; | num_missing &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 2048 | 3 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  num_rows num_cols num_missing\n",
       "1 2048     3        0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"--------- Testing Data Summary ---------\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 x 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>num_rows</th><th scope=col>num_cols</th><th scope=col>num_missing</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1267</td><td>2</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 x 3\n",
       "\\begin{tabular}{lll}\n",
       " num\\_rows & num\\_cols & num\\_missing\\\\\n",
       " <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t 1267 & 2 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 x 3\n",
       "\n",
       "| num_rows &lt;int&gt; | num_cols &lt;int&gt; | num_missing &lt;int&gt; |\n",
       "|---|---|---|\n",
       "| 1267 | 2 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  num_rows num_cols num_missing\n",
       "1 1267     2        0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"--------- Training Data Summary ---------\")\n",
    "summarize(train_data,\n",
    "          num_rows = n(), \n",
    "          num_cols = ncol(train_data),\n",
    "          num_missing = sum(is.na(train_data)))\n",
    "\n",
    "print(\"--------- Validation Set Summary ---------\")\n",
    "summarise(validation_data,\n",
    "          num_rows = n(),\n",
    "          num_cols = ncol(validation_data),\n",
    "          num_missing = sum(is.na(validation_data)))\n",
    "\n",
    "print(\"--------- Testing Data Summary ---------\")\n",
    "summarise(test_data,\n",
    "          num_rows = n(),\n",
    "          num_cols = ncol(test_data),\n",
    "          num_missing = sum(is.na(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "We tokenize all of the datasets such that each datapoint will be a vector of the words it contains. All of the letters are converted into lowercase. The punctuation is removed. Stopwords such as \"and\", \"of\", or \"the\" are also removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
